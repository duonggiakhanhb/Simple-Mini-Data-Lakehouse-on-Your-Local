{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://minioserver:9000\n",
      "http://nessie:19120/api/v1\n",
      "s3a://data-lakehouse/\n",
      "Spark Running\n",
      "The PySpark 3.3.1 version is running...\n",
      "root\n",
      " |-- CustomerID: integer (nullable = true)\n",
      " |-- FirstName: string (nullable = true)\n",
      " |-- LastName: string (nullable = true)\n",
      " |-- DateOfBirth: string (nullable = true)\n",
      " |-- ContactPhone: string (nullable = true)\n",
      " |-- Email: string (nullable = true)\n",
      " |-- Address: string (nullable = true)\n",
      "\n",
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- AccountID: integer (nullable = true)\n",
      " |-- CustomerID: integer (nullable = true)\n",
      " |-- AccountType: string (nullable = true)\n",
      " |-- Balance: double (nullable = true)\n",
      " |-- AccountOpenDate: string (nullable = true)\n",
      "\n",
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CreditCardID: integer (nullable = true)\n",
      " |-- CardNumber: string (nullable = true)\n",
      " |-- CustomerID: integer (nullable = true)\n",
      " |-- CreditLimit: double (nullable = true)\n",
      " |-- OutstandingBalance: double (nullable = true)\n",
      " |-- CardExpiryDate: string (nullable = true)\n",
      "\n",
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvestmentID: integer (nullable = true)\n",
      " |-- CustomerID: integer (nullable = true)\n",
      " |-- InvestmentType: string (nullable = true)\n",
      " |-- InvestmentAmount: double (nullable = true)\n",
      " |-- PurchaseDate: string (nullable = true)\n",
      " |-- CurrentValue: double (nullable = true)\n",
      "\n",
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- TransactionID: integer (nullable = true)\n",
      " |-- AccountID: integer (nullable = true)\n",
      " |-- TransactionDate: string (nullable = true)\n",
      " |-- TransactionType: string (nullable = true)\n",
      " |-- Amount: double (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      "\n",
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/11/16 06:21:25 WARN AbstractConnector: \n",
      "java.io.IOException: No such file or directory\n",
      "\tat java.base/sun.nio.ch.NativeThread.signal(Native Method)\n",
      "\tat java.base/sun.nio.ch.ServerSocketChannelImpl.implCloseSelectableChannel(ServerSocketChannelImpl.java:365)\n",
      "\tat java.base/java.nio.channels.spi.AbstractSelectableChannel.implCloseChannel(AbstractSelectableChannel.java:242)\n",
      "\tat java.base/java.nio.channels.spi.AbstractInterruptibleChannel.close(AbstractInterruptibleChannel.java:112)\n",
      "\tat org.sparkproject.jetty.server.ServerConnector.close(ServerConnector.java:371)\n",
      "\tat org.sparkproject.jetty.server.AbstractNetworkConnector.shutdown(AbstractNetworkConnector.java:104)\n",
      "\tat org.sparkproject.jetty.server.Server.doStop(Server.java:444)\n",
      "\tat org.sparkproject.jetty.util.component.AbstractLifeCycle.stop(AbstractLifeCycle.java:94)\n",
      "\tat org.apache.spark.ui.ServerInfo.stop(JettyUtils.scala:525)\n",
      "\tat org.apache.spark.ui.WebUI.$anonfun$stop$2(WebUI.scala:180)\n",
      "\tat org.apache.spark.ui.WebUI.$anonfun$stop$2$adapted(WebUI.scala:180)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.ui.WebUI.stop(WebUI.scala:180)\n",
      "\tat org.apache.spark.ui.SparkUI.stop(SparkUI.scala:141)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$stop$6(SparkContext.scala:2085)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$stop$6$adapted(SparkContext.scala:2085)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$stop$5(SparkContext.scala:2085)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1484)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2085)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.stop(JavaSparkContext.scala:550)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/11/16 06:22:25 WARN QueuedThreadPool: QueuedThreadPool[SparkUI]@2f2300a8{STOPPING,8<=0<=200,i=7,r=-1,q=0}[NO_TRY] Couldn't stop Thread[SparkUI-367-acceptor-0@2951f6be-ServerConnector@27961de4{HTTP/1.1, (http/1.1)}{0.0.0.0:4042},3,main]\n",
      "Success!\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "\n",
    "## DEFINE SENSITIVE VARIABLES\n",
    "NESSIE_URI = os.environ.get(\"NESSIE_URI\") ## Nessie Server URI\n",
    "DATALAKEHOUSE = os.environ.get(\"DATALAKEHOUSE\") ## BUCKET TO WRITE DATA TOO\n",
    "AWS_ACCESS_KEY = os.environ.get(\"AWS_ACCESS_KEY\") ## AWS CREDENTIALS\n",
    "AWS_SECRET_KEY = os.environ.get(\"AWS_SECRET_KEY\") ## AWS CREDENTIALS\n",
    "AWS_S3_ENDPOINT= os.environ.get(\"AWS_S3_ENDPOINT\") ## MINIO ENDPOINT\n",
    "\n",
    "\n",
    "print(AWS_S3_ENDPOINT)\n",
    "print(NESSIE_URI)\n",
    "print(DATALAKEHOUSE)\n",
    "\n",
    "\n",
    "conf = (\n",
    "    pyspark.SparkConf()\n",
    "        .setAppName('SQLiteToIceberg')\n",
    "        .set('spark.jars.packages', 'org.apache.iceberg:iceberg-spark-runtime-3.3_2.12:1.3.1' +\\\n",
    "        ',org.projectnessie.nessie-integrations:nessie-spark-extensions-3.3_2.12:0.67.0' +\\\n",
    "        ',org.xerial:sqlite-jdbc:3.43.2.2' +\\\n",
    "        ',software.amazon.awssdk:bundle:2.17.178,software.amazon.awssdk:url-connection-client:2.17.178')\n",
    "        .set('spark.sql.extensions', 'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions')\n",
    "        .set('spark.sql.catalog.nessie', 'org.apache.iceberg.spark.SparkCatalog')\n",
    "        .set('spark.sql.catalog.nessie.uri', NESSIE_URI)\n",
    "        .set('spark.sql.catalog.nessie.ref', 'main')\n",
    "        .set('spark.sql.catalog.demo.s3.path-style-access', 'true')\n",
    "        .set('spark.sql.catalog.nessie.authentication.type', 'NONE')\n",
    "        .set('spark.sql.catalog.nessie.catalog-impl', 'org.apache.iceberg.nessie.NessieCatalog')\n",
    "        .set('spark.sql.catalog.nessie.s3.endpoint', AWS_S3_ENDPOINT)\n",
    "        .set('spark.sql.catalog.nessie.warehouse', DATALAKEHOUSE)\n",
    "        .set('spark.sql.catalog.nessie.io-impl', 'org.apache.iceberg.aws.s3.S3FileIO')\n",
    "        .set('spark.hadoop.fs.s3a.access.key', AWS_ACCESS_KEY)\n",
    "        .set('spark.hadoop.fs.s3a.secret.key', AWS_SECRET_KEY)\n",
    ")\n",
    "\n",
    "\n",
    "## Start Spark Session\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "print(\"Spark Running\")\n",
    "print(f'The PySpark {spark.version} version is running...')\n",
    "spark.sql(\"USE nessie\")\n",
    "# Define a list of tables\n",
    "tables = [\"customers\", \"accounts\", \"creditCards\", \"investments\", \"transactions\"]\n",
    "\n",
    "# Loop through the tables\n",
    "for table in tables:\n",
    "    # Load data from SQLite into a Spark DataFrame\n",
    "    sqlite_df = spark.read.format(\"jdbc\") \\\n",
    "        .option(\"url\", \"jdbc:sqlite:db/bank_data.db\") \\\n",
    "        .option(\"dbtable\", f\"(SELECT * FROM {table}) AS tmp\") \\\n",
    "        .option(\"driver\", \"org.sqlite.JDBC\") \\\n",
    "        .load()\n",
    "    \n",
    "    sqlite_df.printSchema()\n",
    "    # Define the Iceberg table name\n",
    "    iceberg_table_name = f\"nessie.{table}\"\n",
    "    ## Create a Table\n",
    "    spark.sql(f\"CREATE TABLE IF NOT EXISTS {iceberg_table_name} (name STRING) USING iceberg;\").show()\n",
    "\n",
    "    # Check if the table exists\n",
    "\n",
    "    sqlite_df.write.format('iceberg').mode('overwrite').saveAsTable(iceberg_table_name)\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n",
    "print(\"Success!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
